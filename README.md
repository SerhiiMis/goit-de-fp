# ğŸ‹ï¸â€â™‚ï¸ Athlete Performance Pipeline (Batch + Streaming)

This project is the final assignment for the **Data Engineering course**, combining:

- Batch data processing (Apache Spark)
- Streaming data ingestion (Apache Kafka)
- Multi-layer architecture (Landing â†’ Bronze â†’ Silver â†’ Gold)
- Automation with Apache Airflow

---

## ğŸ§­ Project Overview

The project is divided into two main parts:

- **Streaming Pipeline** â€“ handles real-time data ingestion using Kafka (producer logic and topic management)
- **Batch Pipeline** â€“ performs ETL using PySpark, orchestrated with Airflow

This repository includes both parts, but the `batch_pipeline/` folder focuses specifically on batch processing and automation logic.

---

## ğŸ“ Project Structure

```
goit-de-fp/
â”œâ”€â”€ streaming_pipeline/
â”‚ â”œâ”€â”€ batch_pipeline/
â”‚ â”‚ â”œâ”€â”€ landing_to_bronze.py
â”‚ â”‚ â”œâ”€â”€ bronze_to_silver.py
â”‚ â”‚ â”œâ”€â”€ silver_to_gold.py
â”‚ â”œâ”€â”€ dags/
â”‚ â”‚ â”œâ”€â”€ batch_pipeline.py
â”‚ â”œâ”€â”€ data/
â”‚ â”‚ â”œâ”€â”€ athlete_bio.csv
â”‚ â”‚ â”œâ”€â”€ athlete_event_results.csv
â”‚ â”œâ”€â”€ output/
â”‚ â”‚ â”œâ”€â”€ bronze/
â”‚ â”‚ â”œâ”€â”€ silver/
â”‚ â”‚ â”œâ”€â”€ gold/
â”‚ â”œâ”€â”€ spark_job.py
â”œâ”€â”€ README.md

```

---

## ğŸ› ï¸ Batch Pipeline (ETL)

The pipeline consists of three main processing stages:

1. `landing_to_bronze.py` â€” reads CSV files from `data/` and stores them as Parquet in `bronze/`
2. `bronze_to_silver.py` â€” joins and cleans data, outputs to `silver/`
3. `silver_to_gold.py` â€” performs aggregation (e.g., weight, height stats) and saves to `gold/avg_stats/`

Execution is automated via the DAG `batch_pipeline.py`.

---

## ğŸ–¥ï¸ Screenshots & Execution Examples

| Stage                 | Description           | Screenshot                                                           |
| --------------------- | --------------------- | -------------------------------------------------------------------- |
| Docker                | Container check       | ![](./streaming_pipeline/screenshots/01_docker_ps.png)               |
| Kafka Topics          | Listing topics        | ![](./streaming_pipeline/screenshots/02_kafka_topics_list.png)       |
| Producer              | Sending messages      | ![](./streaming_pipeline/screenshots/03_producer_running.png)        |
| Kafka kcat            | Message inspection    | ![](./streaming_pipeline/screenshots/04_kcat_messages.png)           |
| Bronze                | Bronze layer creation | ![](./streaming_pipeline/screenshots/05_bronze.png)                  |
| Bronze folder         | Directory contents    | ![](./streaming_pipeline/screenshots/06_bronze_folder.png)           |
| Bronze â†’ Silver       | Transformation step   | ![](./streaming_pipeline/screenshots/07_bronze_to_silver.png)        |
| Silver folder         | Silver Parquet files  | ![](./streaming_pipeline/screenshots/08_bronze_to_silver_folder.png) |
| Silver â†’ Gold         | Aggregation           | ![](./streaming_pipeline/screenshots/09_silver_to_gold.png)          |
| Gold folder           | Gold Parquet files    | ![](./streaming_pipeline/screenshots/10_silver_to_gold_folder.png)   |
| DAG: landing â†’ bronze | Logs                  | ![](./streaming_pipeline/screenshots/11_landing_to_bronze_logs.png)  |
| DAG: bronze â†’ silver  | Logs                  | ![](./streaming_pipeline/screenshots/12_bronze_to_silver_logs.png)   |
| DAG: silver â†’ gold    | Logs                  | ![](./streaming_pipeline/screenshots/13_silver_to_gold_logs.png)     |
| DAG Graph             | DAG visualization     | ![](./streaming_pipeline/screenshots/14_graph.png)                   |

---

## ğŸ”„ Airflow Automation

- The DAG `batch_pipeline.py` orchestrates all Spark jobs sequentially.
- Each task runs via `BashOperator` using `spark-submit`.

---

## ğŸ“Œ Notes

- Dataset: [athlete_events.csv](https://www.kaggle.com/datasets/heesoo37/120-years-of-olympic-history-athletes-and-results)
- Spark runs in local mode (no YARN).
- Kafka is used for streaming (partially implemented).
